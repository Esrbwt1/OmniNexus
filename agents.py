# OmniNexus - agents.py
# Defines the base structure for AI agents and implements specific agents.

from abc import ABC, abstractmethod
import re # For simple word counting
import collections
import heapq # For efficiently finding top N sentences
# NLTK imports
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import sent_tokenize, word_tokenize
    import os # Import os module

    # Ensure data was downloaded (optional check)
    try:
         stopwords.words('english')
         sent_tokenize("Test sentence.")
         print("NLTK data 'punkt' and 'stopwords' seem available.") # Confirmation message
    except LookupError:
         # Keep existing error message block...
         print("="*60)
         print("ERROR: NLTK data ('punkt', 'stopwords') not found.")
         print("Please run 'python -m nltk.downloader -d \"%USERPROFILE%/nltk_data\" stopwords punkt' (Windows)")
         print("or 'python -m nltk.downloader -d \"$HOME/nltk_data\" stopwords punkt' (Linux/macOS)")
         print("or use the interactive downloader in Python: nltk.download(['punkt', 'stopwords'])")
         print(f"Searched paths: {nltk.data.path}") # Show where it searched
         print("="*60)
         raise ImportError("NLTK data not found. Please download.")
    except Exception as e:
         print(f"Warning: Error during NLTK initial check: {e}")

except ImportError:
    # Keep existing ImportError handling...
    print("="*60)
    print("ERROR: NLTK library not found. SummarizationAgent will not work.")
    print("Please install it: pip install nltk")
    print("="*60)
    raise

# --- Base Agent Class ---

class BaseAgent(ABC):
    """
    Abstract Base Class for all OmniNexus AI/computation agents.

    Defines the common interface for agents that process data items retrieved
    by connectors and produce results or insights.

    Subclasses must implement all methods decorated with @abstractmethod
    and the @classmethod get_config_schema.
    """
    def __init__(self, agent_id, config):
        """
        Initializes the agent instance.

        Should be called by subclasses using super().__init__(...).

        :param agent_id: A unique identifier for this specific agent instance (e.g., 'poc_word_counter').
                         Often generated by the orchestrator when the agent is run.
        :param config: A dictionary containing configuration parameters specific
                       to this agent instance (e.g., model path, API key, behavior flags).
                       Must include a 'type' key matching the registered agent type.
                       Can be an empty dict if the agent type requires no config.
        """
        if not agent_id or not isinstance(agent_id, str):
             raise ValueError("Agent ID must be a non-empty string.")
        # Config can be empty if agent needs no configuration beyond its type
        if not isinstance(config, dict):
             raise ValueError("Agent config must be a dictionary (can be empty if agent type requires no config).")

        self.agent_id = agent_id
        # Make a copy to prevent external modification
        self.config = config.copy()
        # Validate the specific config keys required by the subclass
        try:
            self.validate_config()
        except (ValueError, TypeError) as e:
            # Add context to validation errors
            raise type(e)(f"Configuration validation failed for agent '{agent_id}' (type '{config.get('type', 'N/A')}'): {e}")


    @abstractmethod
    def validate_config(self):
        """
        Validates the self.config dictionary against the requirements of this specific agent type.

        - Should check for the presence and correct types of required keys (if any).
        - Should validate values if necessary.
        - May assign default values from the schema to self.config if optional keys are missing.
        - Should raise ValueError or TypeError for invalid configurations.
        - If an agent requires no configuration beyond its type, this method can simply `pass`.
        """
        pass

    @abstractmethod
    def get_metadata(self):
        """
        Returns metadata about the agent instance and its capabilities.

        :return: A dictionary containing information such as:
                 - agent_id: str
                 - type: str (e.g., 'word_counter', 'summarizer')
                 - description: str (User-friendly explanation of what the agent does)
                 - input_format: str (Description of the expected `data_inputs` format, usually referencing `protocol.DATA_ITEM_STRUCTURE`)
                 - output_format: str (Description of the format of the dictionary returned by `execute`)
                 - config_schema: dict (Optional: The schema definition from `get_config_schema` can be included here)
        """
        pass

    @abstractmethod
    def execute(self, data_inputs, parameters=None):
        """
        The core method where the agent performs its computation or task.

        :param data_inputs: A list of dictionaries, where each dictionary conforms to
                            `protocol.DATA_ITEM_STRUCTURE`. These are typically the outputs
                            from one or more connector queries.
        :param parameters: Optional dictionary containing parameters specific to *this execution run*.
                           These can override the agent's base configuration (from `self.config`)
                           for this specific task (e.g., asking for a different number of keywords).
                           The agent implementation decides how to interpret and use these parameters.

        :return: A dictionary containing the results of the agent's computation.
                 The structure of this dictionary is defined by the specific agent implementation
                 (and should be described in `get_metadata`). Should include status indicators
                 (e.g., items processed/skipped) and potentially error information if applicable.
        """
        pass

    @classmethod
    @abstractmethod
    def get_config_schema(cls):
        """
        Returns a schema describing the configuration parameters required *at creation time*
        for this agent type (if any).

        This is used by the UI/CLI or orchestrator if agents need persistent configuration
        beyond just their type (e.g., path to a local model file, API key).
        If an agent type requires no specific configuration to be created, this
        method should return an empty dictionary `{}`.

        :return: A dictionary adhering to `protocol.CONFIG_SCHEMA_FORMAT`, or `{}`.
        """
        pass

# --- Specific Agent Implementations ---

class WordCountAgent(BaseAgent):
    """
    A simple agent that counts words in the text data provided via
    standard Data Item format (Phase 2).
    """

    @classmethod
    def get_config_schema(cls):
        return {}

    def validate_config(self):
        if self.config and self.config != {'type': 'word_counter'}: # Allow the type key from factory
             print(f"Warning: WordCountAgent '{self.agent_id}' received unexpected config: {self.config}")
        pass

    def get_metadata(self):
        """Returns metadata about the WordCountAgent."""
        return {
            "agent_id": self.agent_id,
            "type": "word_counter",
            "description": "Counts the total number of words in the 'content' field of the payload within standard Data Items.",
            "input_format": "List[Dict] conforming to protocol.DATA_ITEM_STRUCTURE with text in payload['content']",
            "output_format": "Dictionary {'total_words': count, 'items_processed': N, 'items_skipped': M}"
        }

    def execute(self, data_inputs, parameters=None):
        """
        Counts words in payload['content'] of each valid Data Item.
        :param data_inputs: List[Dict] conforming to protocol.DATA_ITEM_STRUCTURE
        :param parameters: Ignored by this agent.
        :return: Dictionary {'total_words': count, 'items_processed': N, 'items_skipped': M}
        """
        total_words = 0
        items_processed = 0
        items_skipped = 0

        if not isinstance(data_inputs, list):
             print(f"Error: WordCountAgent expects a list of data inputs, got {type(data_inputs)}")
             return {"total_words": 0, "items_processed": 0, "items_skipped": len(data_inputs) if isinstance(data_inputs, list) else 1 , "error": "Invalid input format: Expected list"}

        print(f"WordCountAgent '{self.agent_id}' executing on {len(data_inputs)} data inputs...")

        for item in data_inputs:
            # Validate basic structure
            if not isinstance(item, dict) or 'payload' not in item or not isinstance(item['payload'], dict) or 'content' not in item['payload']:
                print(f"Warning: Skipping item due to missing structure (payload or payload['content']): {item.get('item_id', 'N/A')}")
                items_skipped += 1
                continue

            content = item['payload']['content']
            if isinstance(content, str):
                # Simple word count using regex
                words = re.findall(r'\b\w+\b', content)
                count = len(words)
                total_words += count
                items_processed += 1
                # Debug print (optional):
                # source = item.get('source_uri', item.get('item_id', 'N/A'))
                # print(f"  - Processed item {source} with {count} words.")
            else:
                print(f"Warning: Skipping item {item.get('item_id', 'N/A')} because payload['content'] is not a string: {type(content)}")
                items_skipped += 1

        print(f"WordCountAgent '{self.agent_id}' finished execution. Words: {total_words}, Processed: {items_processed}, Skipped: {items_skipped}")
        return {
            "total_words": total_words,
            "items_processed": items_processed,
            "items_skipped": items_skipped
        }


# Basic English stop words list (can be expanded significantly)
STOP_WORDS = set([
    "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "aren't", "as", "at",
    "be", "because", "been", "before", "being", "below", "between", "both", "but", "by",
    "can't", "cannot", "could", "couldn't", "did", "didn't", "do", "does", "doesn't", "doing", "don't", "down", "during",
    "each", "few", "for", "from", "further", "had", "hadn't", "has", "hasn't", "have", "haven't", "having", "he", "he'd",
    "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's",
    "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "isn't", "it", "it's", "its", "itself",
    "let's", "me", "more", "most", "mustn't", "my", "myself",
    "no", "nor", "not", "of", "off", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own",
    "same", "shan't", "she", "she'd", "she'll", "she's", "should", "shouldn't", "so", "some", "such",
    "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd",
    "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very",
    "was", "wasn't", "we", "we'd", "we'll", "we're", "we've", "were", "weren't", "what", "what's", "when", "when's", "where",
    "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "won't", "would", "wouldn't",
    "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves",
    # Consider adding common short words or context-specific words if needed
    "fig", "figure", "table", "also", "data", "using", "used" # Example additions
])

class KeywordExtractAgent(BaseAgent):
    """
    A simple agent that extracts potential keywords based on word frequency,
    after removing common stop words. Uses standard Data Items (Phase 2).
    """

    @classmethod
    def get_config_schema(cls):
        return {
            "num_keywords": {"type": "integer", "required": False, "default": 10, "description": "Maximum number of keywords to return per execution."},
            "min_word_length": {"type": "integer", "required": False, "default": 3, "description": "Minimum length of a word to be considered a keyword."}
        }

    def validate_config(self):
        """Validate configuration."""
        schema = self.get_config_schema()
        num_k = self.config.get("num_keywords", schema['num_keywords']['default'])
        min_len = self.config.get("min_word_length", schema['min_word_length']['default'])

        if not isinstance(num_k, int) or num_k <= 0:
            raise ValueError("'num_keywords' must be a positive integer.")
        if not isinstance(min_len, int) or min_len < 1:
            raise ValueError("'min_word_length' must be a positive integer >= 1.")

        # Ensure defaults are set in the instance config if not provided
        self.config['num_keywords'] = num_k
        self.config['min_word_length'] = min_len

    def get_metadata(self):
        """Returns metadata about the KeywordExtractAgent."""
        return {
            "agent_id": self.agent_id,
            "type": "keyword_extractor",
            "description": "Extracts keywords from text content in Data Items based on frequency (excluding stop words).",
            "config_schema": self.get_config_schema(),
            "input_format": "List[Dict] conforming to protocol.DATA_ITEM_STRUCTURE with text in payload['content']",
            "output_format": "Dictionary {'keywords': [{'word': w, 'score': count}, ...], 'items_processed': N, 'items_skipped': M}"
        }

    def execute(self, data_inputs, parameters=None):
        """
        Extracts keywords from relevant text fields within the payload
        of each valid Data Item. Prioritizes 'content', falls back to
        'subject' and 'from'.
        :param data_inputs: List[Dict] conforming to protocol.DATA_ITEM_STRUCTURE
        :param parameters: Optional dictionary, potentially overriding config for this run (e.g., {"num_keywords": 5}). Ignored if not provided.
        :return: Dictionary {'keywords': List[Dict{'word', 'score'}], 'items_processed': N, 'items_skipped': M}
        """
        items_processed = 0
        items_skipped = 0
        word_counts = collections.Counter()

        # Determine parameters for this run (override config if provided)
        run_params = self.config.copy() # Start with agent's base config
        if isinstance(parameters, dict):
            num_k_override = parameters.get("num_keywords")
            min_len_override = parameters.get("min_word_length")
            try:
                if num_k_override is not None: run_params['num_keywords'] = int(num_k_override)
                if min_len_override is not None: run_params['min_word_length'] = int(min_len_override)
                if run_params['num_keywords'] <= 0: raise ValueError("num_keywords must be positive")
                if run_params['min_word_length'] < 1: raise ValueError("min_word_length must be >= 1")
            except (ValueError, TypeError) as e:
                print(f"Warning: Invalid execution parameter provided, using agent default config. Error: {e}")
                run_params = self.config.copy() # Revert to original on error

        num_keywords_to_return = run_params['num_keywords']
        min_word_length = run_params['min_word_length']

        print(f"KeywordExtractAgent '{self.agent_id}' executing on {len(data_inputs)} data inputs...")
        print(f"  Parameters: num_keywords={num_keywords_to_return}, min_word_length={min_word_length}")

        if not isinstance(data_inputs, list):
            print(f"Error: KeywordExtractAgent expects a list, got {type(data_inputs)}")
            return {"keywords": [], "items_processed": 0, "items_skipped": len(data_inputs) if isinstance(data_inputs, list) else 1, "error": "Invalid input format: Expected list"}

        for item in data_inputs:
            text_to_process = None # Initialize text source for this item

            # Validate basic structure - check for payload dictionary first
            if not isinstance(item, dict) or 'payload' not in item or not isinstance(item['payload'], dict):
                print(f"Warning: Skipping item due to missing or invalid 'payload': {item.get('item_id', 'N/A')}")
                items_skipped += 1
                continue

            payload = item['payload']

            # Prioritize 'content' key if it exists and is a string
            if isinstance(payload.get('content'), str):
                text_to_process = payload['content']
            # Fallback: If no 'content', try combining 'subject' and 'from'
            elif not text_to_process:
                subject = payload.get('subject', '')
                sender = payload.get('from', '')
                # Ensure they are strings before concatenating
                if isinstance(subject, str) or isinstance(sender, str):
                    combined_text = f"{str(subject)} {str(sender)}" # Combine available text
                    if combined_text.strip(): # Check if there's actually text after combining
                        text_to_process = combined_text
                        # print(f"Debug: Using combined subject/from for item {item.get('item_id', 'N/A')}") # Optional debug
                    else:
                        print(f"Warning: Skipping item {item.get('item_id', 'N/A')} - No 'content', and 'subject'/'from' are empty/invalid.")
                        items_skipped += 1
                        continue
                else:
                    print(f"Warning: Skipping item {item.get('item_id', 'N/A')} - No 'content', and 'subject'/'from' payload keys are missing or not strings.")
                    items_skipped += 1
                    continue

            # Check if we successfully got text to process for this item
            if text_to_process is not None and isinstance(text_to_process, str):
                # Naive keyword extraction: lowercase, split, filter stop words & length
                words = re.findall(r'\b\w+\b', text_to_process.lower())
                potential_keywords = [
                    word for word in words
                    if word not in STOP_WORDS and len(word) >= min_word_length
                ]
                word_counts.update(potential_keywords)
                items_processed += 1
            # This else shouldn't be strictly necessary due to checks above, but acts as a safeguard
            elif text_to_process is None:
                # Warning message already printed in the logic above
                # items_skipped counter already incremented
                pass
            else: # Should not happen if logic above is correct, but catch non-string case
                print(f"Warning: Skipping item {item.get('item_id', 'N/A')} - derived text_to_process was not a string: {type(text_to_process)}")
                items_skipped += 1


        # Get the most common keywords
        most_common = word_counts.most_common(num_keywords_to_return)

        # Format output
        result_keywords = [{"word": word, "score": count} for word, count in most_common]

        print(f"KeywordExtractAgent '{self.agent_id}' finished. Found {len(result_keywords)} keywords. Processed: {items_processed}, Skipped: {items_skipped}")
        return {
            "keywords": result_keywords,
            "items_processed": items_processed,
            "items_skipped": items_skipped
        }

class SummarizationAgent(BaseAgent):
    """
    A simple agent that performs extractive summarization on text content.
    It ranks sentences based on the frequency of non-stop words they contain.
    Uses standard Data Items (Phase 2/3). Requires NLTK library.
    """

    @classmethod
    def get_config_schema(cls):
        return {
            "summary_sentences": {"type": "integer", "required": False, "default": 3, "description": "Number of sentences to include in the summary."},
            # Future: Could add language config for stop words
        }

    def validate_config(self):
        """Validate configuration."""
        schema = self.get_config_schema()
        num_sentences = self.config.get("summary_sentences", schema['summary_sentences']['default'])

        if not isinstance(num_sentences, int) or num_sentences <= 0:
            raise ValueError("'summary_sentences' must be a positive integer.")

        self.config['summary_sentences'] = num_sentences

    def get_metadata(self):
        """Returns metadata about the SummarizationAgent."""
        return {
            "agent_id": self.agent_id,
            "type": "summarizer",
            "description": "Extracts a summary by selecting top-ranked sentences based on word frequency (requires NLTK).",
            "config_schema": self.get_config_schema(),
            "input_format": "List[Dict] conforming to protocol.DATA_ITEM_STRUCTURE with text in payload['content'] or fallback fields.",
            "output_format": "Dictionary {'summary': str, 'items_processed': N, 'items_skipped': M}"
        }

    def _get_text_from_item(self, item):
         """Helper to extract text, prioritizing content, fallback to subject/from."""
         if not isinstance(item, dict) or 'payload' not in item or not isinstance(item['payload'], dict):
             return None # Invalid structure

         payload = item['payload']
         text = payload.get('content')
         if isinstance(text, str) and text.strip():
             return text

         # Fallback for emails etc.
         subject = payload.get('subject')
         sender = payload.get('from') # 'from' is a reserved keyword, use get()
         combined = []
         if isinstance(subject, str) and subject.strip(): combined.append(subject)
         # Potentially add body_text if available and content was None/empty? Maybe too complex for now.
         # if isinstance(payload.get('body_text'), str): combined.append(payload['body_text'])
         if isinstance(sender, str) and sender.strip(): combined.append(sender)

         if combined:
              return " ".join(combined) # Combine subject/from
         else:
              return None # No usable text found

    def execute(self, data_inputs, parameters=None):
        """
        Generates an extractive summary from the text content of input items.
        """
        items_processed = 0
        items_skipped = 0
        full_text = ""

        # Determine parameters for this run
        run_params = self.config.copy()
        if isinstance(parameters, dict):
            num_sent_override = parameters.get("summary_sentences")
            try:
                if num_sent_override is not None: run_params['summary_sentences'] = int(num_sent_override)
                if run_params['summary_sentences'] <= 0: raise ValueError("summary_sentences must be positive")
            except (ValueError, TypeError) as e:
                print(f"Warning: Invalid execution parameter 'summary_sentences', using agent default. Error: {e}")
                run_params = self.config.copy() # Revert

        num_sentences_to_return = run_params['summary_sentences']

        print(f"SummarizationAgent '{self.agent_id}' executing on {len(data_inputs)} data inputs...")
        print(f"  Parameters: summary_sentences={num_sentences_to_return}")

        if not isinstance(data_inputs, list):
             print(f"Error: SummarizationAgent expects a list, got {type(data_inputs)}")
             return {"summary": "", "items_processed": 0, "items_skipped": len(data_inputs) if isinstance(data_inputs, list) else 1, "error": "Invalid input format: Expected list"}

        # 1. Concatenate text from all items
        for item in data_inputs:
             item_text = self._get_text_from_item(item)
             if item_text:
                  full_text += item_text + "\n\n" # Add separators between item texts
                  items_processed += 1
             else:
                  print(f"Warning: Skipping item {item.get('item_id', 'N/A')} - Could not extract usable text.")
                  items_skipped += 1

        if not full_text.strip() or items_processed == 0:
             print("No text content found to summarize.")
             return {"summary": "", "items_processed": items_processed, "items_skipped": items_skipped}

        # 2. Calculate word frequencies (excluding stop words)
        stop_words_list = stopwords.words('english')
        # Add common punctuation to stop words for frequency calculation
        stop_words_list.extend(['.', ',', '!', '?', ';', ':', '(', ')', '[', ']', '{', '}', '--', '-'])
        word_frequencies = collections.Counter()
        # Use word_tokenize which handles punctuation better than basic split or regex for this purpose
        try:
             words = word_tokenize(full_text.lower())
             for word in words:
                  if word not in stop_words_list:
                       word_frequencies[word] += 1
        except Exception as e:
             print(f"Error during word tokenization/frequency calculation: {e}")
             return {"summary": "", "items_processed": items_processed, "items_skipped": items_skipped, "error": "Tokenization failed"}

        # Find max frequency for normalization (optional but can help)
        max_freq = max(word_frequencies.values()) if word_frequencies else 0

        # 3. Tokenize into sentences
        try:
             sentences = sent_tokenize(full_text)
        except Exception as e:
              print(f"Error during sentence tokenization: {e}")
              return {"summary": "", "items_processed": items_processed, "items_skipped": items_skipped, "error": "Sentence tokenization failed"}

        if not sentences:
             print("Could not tokenize text into sentences.")
             return {"summary": "", "items_processed": items_processed, "items_skipped": items_skipped}

        # 4. Calculate sentence scores
        sentence_scores = {}
        for i, sentence in enumerate(sentences):
             try:
                  sentence_words = word_tokenize(sentence.lower())
                  score = 0
                  for word in sentence_words:
                       if word in word_frequencies:
                            score += word_frequencies[word]
                  # Normalize score by sentence length (optional, favors shorter high-freq sentences)
                  # score = score / len(sentence_words) if sentence_words else 0
                  sentence_scores[i] = score # Store score by original index
             except Exception as e:
                  print(f"Warning: Could not score sentence {i}: {e}")
                  sentence_scores[i] = 0 # Assign 0 score if processing fails

        # 5. Select top N sentences using heapq
        # heapq.nlargest returns list of items (scores), we need the original sentences
        # Get indices of top N scores
        top_sentence_indices = heapq.nlargest(num_sentences_to_return, sentence_scores, key=sentence_scores.get)

        # Sort the top indices so the summary reads in original order
        top_sentence_indices.sort()

        # 6. Build the summary string
        summary = " ".join([sentences[i] for i in top_sentence_indices])

        print(f"SummarizationAgent '{self.agent_id}' finished. Generated summary with {len(top_sentence_indices)} sentences. Processed: {items_processed}, Skipped: {items_skipped}")
        return {
            "summary": summary,
            "items_processed": items_processed,
            "items_skipped": items_skipped
        }


# --- Agent Registry and Factory ---

# Add the new agent class to the registry dictionary
_agent_types = {
    "word_counter": WordCountAgent,
    "keyword_extractor": KeywordExtractAgent,
    "summarizer": SummarizationAgent # Add the new type here
}


def get_available_agent_types():
    """Returns a list of registered agent type names."""
    return list(_agent_types.keys())

def get_agent_class(agent_type):
    """Gets the class for a given agent type name."""
    return _agent_types.get(agent_type)

def create_agent_instance(agent_id, config):
    """
    Factory function to create agent instances based on config type.
    :param agent_id: Unique ID for this instance.
    :param config: Dictionary containing configuration, MUST include 'type'.
    :return: An instance of a BaseAgent subclass, or None if type is invalid/missing.
    """
    if not config or not isinstance(config, dict):
        print("Error: Agent config must be a dictionary.")
        return None

    agent_type = config.get("type")
    if not agent_type:
        print("Error: Agent config must include a 'type' key.")
        return None

    AgentClass = get_agent_class(agent_type)
    if AgentClass:
        try:
            # Instantiate the specific agent class
            instance = AgentClass(agent_id=agent_id, config=config)
            print(f"Successfully created agent instance '{agent_id}' of type '{agent_type}'.")
            return instance
        except (ValueError, TypeError) as e:
            print(f"Error creating agent '{agent_id}' of type '{agent_type}': Invalid config - {e}")
            return None
        except Exception as e:
            print(f"Unexpected error creating agent '{agent_id}': {e}")
            return None
    else:
        print(f"Error: Unknown agent type '{agent_type}'. Available types: {get_available_agent_types()}")
        return None


# Example of how to use it (will be called from main.py later)
if __name__ == "__main__":
    print("Running agents module directly for testing...")

    # --- Test WordCountAgent ---
    print("\n--- Testing WordCountAgent ---")

    # 1. Test valid creation
    valid_config = {"type": "word_counter"} # No extra config needed
    agent = create_agent_instance("wc_agent_01", valid_config)

    if agent:
        print("Metadata:", agent.get_metadata())

        # 2. Test execution with sample data
        sample_data = [
            {"content": "This is the first piece of text.", "source": "doc1"},
            {"content": "Here is some more text, \n with line breaks.", "source": "doc2"},
            {"content": "", "source": "empty_doc"}, # Empty content
            {"content": "Punctuation! Should? Be Handled.", "source": "doc3"},
            {"not_content": "Invalid format", "source": "bad_doc"}, # Missing 'content' key
             123 # Invalid list item
        ]
        print("\nExecuting agent with sample data...")
        result = agent.execute(sample_data)
        print("Execution Result:", result)

        # Expected word count:
        # "This is the first piece of text." -> 7 words
        # "Here is some more text, with line breaks." -> 8 words (simple split)
        # "" -> 0 words
        # "Punctuation Should Be Handled" -> 4 words (using regex \b\w+\b)
        # Total = 7 + 8 + 0 + 4 = 19

    # 3. Test creation with invalid type
    print("\n--- Testing Invalid Type ---")
    invalid_config_type = {"type": "non_existent_agent"}
    create_agent_instance("test_invalid_type", invalid_config_type)